{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbm829/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/dbm829/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from omicsdgd import DGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "fraction_unpaired = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# load data\n",
    "###\n",
    "data_name = \"human_bonemarrow\"\n",
    "adata = ad.read_h5ad(\"../../data/\" + data_name + \".h5ad\")\n",
    "adata.X = adata.layers[\"counts\"] # I seem to have to do it again\n",
    "\n",
    "# train-validation-test split for reproducibility\n",
    "# best provided as list [[train_indices], [validation_indices]]\n",
    "train_val_split = [\n",
    "    list(np.where(adata.obs[\"train_val_test\"] == \"train\")[0]),\n",
    "    list(np.where(adata.obs[\"train_val_test\"] == \"validation\")[0]),\n",
    "]\n",
    "\n",
    "valset = adata[adata.obs[\"train_val_test\"] == \"validation\"].copy()\n",
    "valset.obs[\"modality\"] = \"paired\"\n",
    "testset = adata[adata.obs[\"train_val_test\"] == \"test\"].copy()\n",
    "testset.obs[\"modality\"] = \"paired\"\n",
    "#adata = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 6925 × 129921\n",
       "    obs: 'GEX_pct_counts_mt', 'GEX_n_counts', 'GEX_n_genes', 'GEX_size_factors', 'GEX_phase', 'ATAC_nCount_peaks', 'ATAC_atac_fragments', 'ATAC_reads_in_peaks_frac', 'ATAC_blacklist_fraction', 'ATAC_nucleosome_signal', 'cell_type', 'batch', 'ATAC_pseudotime_order', 'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality', 'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType', 'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker', 'train_val_test', 'observable', 'covariate_Site', 'modality'\n",
       "    var: 'feature_types', 'gene_id', 'modality'\n",
       "    uns: 'ATAC_gene_activity_var_names', 'dataset_id', 'genome', 'organism'\n",
       "    obsm: 'ATAC_gene_activity', 'ATAC_lsi_full', 'ATAC_lsi_red', 'ATAC_umap', 'GEX_X_pca', 'GEX_X_umap'\n",
       "    layers: 'counts'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpaired = pd.read_csv('../../data/'+data_name+'_unpairing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_1_indices = df_unpaired[\n",
    "    (df_unpaired[\"fraction_unpaired\"] == fraction_unpaired) & (df_unpaired[\"modality\"] == \"rna\")\n",
    "][\"sample_idx\"].values\n",
    "mod_2_indices = df_unpaired[\n",
    "    (df_unpaired[\"fraction_unpaired\"] == fraction_unpaired) & (df_unpaired[\"modality\"] == \"atac\")\n",
    "][\"sample_idx\"].values\n",
    "remaining_indices = df_unpaired[\n",
    "    (df_unpaired[\"fraction_unpaired\"] == fraction_unpaired) & (df_unpaired[\"modality\"] == \"paired\")\n",
    "][\"sample_idx\"].values\n",
    "\n",
    "var_before = adata.var.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied rna\n",
      "copied atac\n",
      "copied rest\n",
      "freed some memory\n",
      "organized data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 56714 × 129921\n",
       "    obs: 'GEX_pct_counts_mt', 'GEX_n_counts', 'GEX_n_genes', 'GEX_size_factors', 'GEX_phase', 'ATAC_nCount_peaks', 'ATAC_atac_fragments', 'ATAC_reads_in_peaks_frac', 'ATAC_blacklist_fraction', 'ATAC_nucleosome_signal', 'cell_type', 'batch', 'ATAC_pseudotime_order', 'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality', 'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType', 'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker', 'train_val_test', 'observable', 'covariate_Site', 'modality'\n",
       "    obsm: 'ATAC_gene_activity', 'ATAC_lsi_full', 'ATAC_lsi_red', 'ATAC_umap', 'GEX_X_pca', 'GEX_X_umap'\n",
       "    layers: 'counts'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adata_unpaired = ad.read(\"../../data/\"+data_name+\"_unpaired-\"+str(fraction_unpaired)+\".h5ad\")\n",
    "\n",
    "adata_rna = adata[mod_1_indices, adata.var[\"feature_types\"] == \"GEX\"].copy()\n",
    "adata_rna.obs[\"modality\"] = \"GEX\"\n",
    "print(\"copied rna\")\n",
    "adata_atac = adata[mod_2_indices, adata.var[\"feature_types\"] == \"ATAC\"].copy()\n",
    "adata_atac.obs[\"modality\"] = \"ATAC\"\n",
    "print(\"copied atac\")\n",
    "adata_multi = adata[remaining_indices, :].copy()\n",
    "adata_multi.obs[\"modality\"] = \"paired\"\n",
    "print(\"copied rest\")\n",
    "adata = None\n",
    "print(\"freed some memory\")\n",
    "adata_unpaired = ad.concat([adata_multi, adata_rna, adata_atac], join=\"outer\")\n",
    "print(\"organized data\")\n",
    "\n",
    "adata_unpaired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_rna, adata_atac, adata_multi = None, None, None\n",
    "adata_unpaired.var = var_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freed memory\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "adata_rna, adata_atac, adata_multi = None, None, None\n",
    "print(\"freed memory\")\n",
    "#adata = adata_unpaired.concatenate(valset)\n",
    "adata = ad.concat([adata_unpaired, valset], join=\"inner\")\n",
    "print(\"finished data\")\n",
    "adata.var = var_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'obs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_val_split \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mwhere(\u001b[43madata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_val_test\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mwhere(adata\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_val_test\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m      4\u001b[0m ]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'obs'"
     ]
    }
   ],
   "source": [
    "train_val_split = [\n",
    "    list(np.where(adata.obs[\"train_val_test\"] == \"train\")[0]),\n",
    "    list(np.where(adata.obs[\"train_val_test\"] == \"validation\")[0]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbm829/Library/Python/3.9/lib/python/site-packages/anndata/_core/anndata.py:1785: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  [AnnData(sparse.csr_matrix(a.shape), obs=a.obs) for a in all_adatas],\n",
      "/Users/dbm829/Library/Python/3.9/lib/python/site-packages/anndata/_core/anndata.py:1785: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  [AnnData(sparse.csr_matrix(a.shape), obs=a.obs) for a in all_adatas],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Gaussian_mix_compture:\n",
      "            Dimensionality: 2\n",
      "            Number of components: 4\n",
      "        \n",
      "#######################\n",
      "Training status\n",
      "#######################\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model = DGD.load(\n",
    "    #data=adata[train_val_split[0]], \n",
    "    data=adata_unpaired,\n",
    "    save_dir=\"../results/trained_models/\" + data_name + \"/\", \n",
    "    model_name=data_name + \"_l20_h2-3_rs\" + str(random_seed)+\"_mosaic\"+str(fraction_unpaired)+\"percent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from omicsdgd.latent import RepresentationLayer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from omicsdgd.functions._predict import prepare_potential_reps, find_new_component, reshape_scaling_factor\n",
    "\n",
    "def learn_new_representations(\n",
    "    gmm,\n",
    "    decoder,\n",
    "    data_loader,\n",
    "    n_samples_new,\n",
    "    correction_model=None,\n",
    "    n_epochs=10,\n",
    "    lrs=[0.01, 0.01],\n",
    "    resampling_type=\"mean\",\n",
    "    resampling_samples=1,\n",
    "    include_correction_error=True,\n",
    "    indices_of_new_distribution=None,\n",
    "    start_from_zero=[False,False],\n",
    "    init_covariate_supervised=None,\n",
    "    supervised=False,\n",
    "    cov_beta=1\n",
    "):\n",
    "    \"\"\"\n",
    "    this function creates samples from the trained GMM for each new point,\n",
    "    returns the best representation for each sample,\n",
    "    and trains the representations with all remaining parameters fixed\n",
    "\n",
    "    gmm: the trained GMM\n",
    "    decoder: the trained decoder\n",
    "    data_loader: the data loader for the data to be predicted\n",
    "    n_samples_new: the number of samples in the new data\n",
    "    correction_model: the trained correction model (if available)\n",
    "    n_epochs: the number of epochs to train the representations\n",
    "    lrs: the learning rates for the representations\n",
    "    resampling_type: the type of resampling to use for the GMM (can be mean or sample)\n",
    "    resampling_samples: the number of samples to draw from the GMM for each new point (is ignored for resampling_type='mean')\n",
    "    \"\"\"\n",
    "\n",
    "    # check if there are unknown distributions in the data\n",
    "    #if gmm.n_mix_comp < len(data_loader.dataset.meta.unique()):\n",
    "    #    print(\"WARNING: there are unknown distributions in the data\\nWill learn extra components in the GMM\")\n",
    "    #    print(gmm.n_mix_comp, len(data_loader.dataset.meta.unique())) # apparently site1 had ha whole cell type\n",
    "    correction_hook = False\n",
    "    #\"\"\"\n",
    "    if correction_model is not None:\n",
    "        if indices_of_new_distribution is not None:\n",
    "            if correction_model.n_mix_comp < data_loader.dataset.correction_classes:\n",
    "                print(\"WARNING: there are unknown distributions in the data\\nWill learn extra components in the batch GMM\")\n",
    "                n_correction_classes_old = correction_model.n_mix_comp\n",
    "                correction_model = find_new_component(\n",
    "                    data_loader,\n",
    "                    decoder,\n",
    "                    correction_model,\n",
    "                    indices_of_new_distribution,\n",
    "                    other_gmm=gmm)\n",
    "                correction_hook = True\n",
    "    #\"\"\"\n",
    "\n",
    "    # make temporary representations with samples from each component per data point\n",
    "    if correction_model is not None:\n",
    "        if not start_from_zero[1]:\n",
    "            if init_covariate_supervised is not None:\n",
    "                potential_reps = prepare_potential_reps(\n",
    "                    [gmm.sample_new_points(resampling_type, resampling_samples), torch.zeros((resampling_samples, correction_model.dim))]\n",
    "                )\n",
    "            else:\n",
    "                potential_reps = prepare_potential_reps(\n",
    "                    [\n",
    "                        gmm.sample_new_points(resampling_type, resampling_samples),\n",
    "                        correction_model.sample_new_points(resampling_type, resampling_samples),\n",
    "                    ]\n",
    "                )\n",
    "        else:\n",
    "            potential_reps = prepare_potential_reps(\n",
    "                [gmm.sample_new_points(resampling_type, resampling_samples), torch.zeros((resampling_samples, correction_model.dim))]\n",
    "            )\n",
    "    else:\n",
    "        potential_reps = prepare_potential_reps(\n",
    "            [gmm.sample_new_points(resampling_type, resampling_samples)]\n",
    "        )\n",
    "\n",
    "    #print(\"making potential reps\")\n",
    "    print(\"   all potential reps: \", potential_reps.shape)\n",
    "\n",
    "    decoder.eval()\n",
    "\n",
    "    ############################\n",
    "    # first match potential reps to samples\n",
    "    ############################\n",
    "    print(\"calculating losses for each new sample and potential reps\")\n",
    "    # creating a storage tensor into which the best reps are copied\n",
    "    rep_init_values = torch.zeros((n_samples_new, potential_reps.shape[-1]))\n",
    "    print(\"   rep_init_values: \", rep_init_values.shape)\n",
    "    # compute predictions for all potential reps\n",
    "    predictions = decoder(potential_reps.to(device))\n",
    "    # go through data loader to calculate losses batch-wise\n",
    "    for x, lib, i in data_loader:\n",
    "        x = x.unsqueeze(1).to(device)\n",
    "        lib = lib.to(device)\n",
    "        if data_loader.dataset.modality_switch is not None:\n",
    "            recon_loss_x = decoder.loss(\n",
    "                [predictions[comp].unsqueeze(0) for comp in range(len(predictions))],\n",
    "                [x[:, :, : data_loader.dataset.modality_switch], x[:, :, data_loader.dataset.modality_switch :]],\n",
    "                scale=[reshape_scaling_factor(lib[:, xxx], 3) for xxx in range(decoder.n_out_groups)],\n",
    "                reduction=\"sample\",\n",
    "                mask=data_loader.dataset.get_mask(i)\n",
    "            )\n",
    "        else:\n",
    "            recon_loss_x = decoder.loss(\n",
    "                [predictions[0].unsqueeze(0)],\n",
    "                [x],\n",
    "                scale=[reshape_scaling_factor(lib[:, 0], 3)],\n",
    "                reduction=\"sample\",\n",
    "                mask=data_loader.dataset.get_mask(i)\n",
    "            )\n",
    "        best_fit_ids = torch.argmin(recon_loss_x, dim=-1).detach().cpu()\n",
    "        rep_init_values[i, :] = potential_reps.clone()[best_fit_ids, :]\n",
    "    # print the counts of how often each component has been chosen\n",
    "    print(\"   \", rep_init_values.mean(0).shape, rep_init_values.mean(-1).shape)\n",
    "    print(\"   counts of how often each component has been chosen: \", np.unique(rep_init_values.mean(-1).numpy(), return_counts=True))\n",
    "\n",
    "    ############################\n",
    "    # create new initial representation\n",
    "    ############################\n",
    "    # create a new representation from the best components\n",
    "    new_rep = RepresentationLayer(n_rep=gmm.dim, n_sample=n_samples_new, value_init=rep_init_values[:, : gmm.dim]).to(\n",
    "        device\n",
    "    )\n",
    "    newrep_optimizer = torch.optim.Adam(new_rep.parameters(), lr=lrs[0], weight_decay=1e-4, betas=(0.5, 0.7))\n",
    "    test_correction_rep = None\n",
    "    if correction_model is not None:\n",
    "        if (not start_from_zero[1]) and (init_covariate_supervised is None):\n",
    "            test_correction_rep = RepresentationLayer(\n",
    "                n_rep=2, n_sample=n_samples_new, value_init=rep_init_values[:, gmm.dim :]\n",
    "            ).to(device)\n",
    "        elif init_covariate_supervised is not None:\n",
    "            if indices_of_new_distribution is not None:\n",
    "                init_covariate_supervised = np.array(init_covariate_supervised)\n",
    "                if (len(np.unique(init_covariate_supervised)) > correction_model.n_mix_comp):\n",
    "                    raise NotImplementedError(\"I can currently only handle one new covariate class\")\n",
    "                #print(init_covariate_supervised[indices_of_new_distribution])\n",
    "                # change the indices of the new distribution to the last component\n",
    "                # count and print the number of unique values in the init covariate supervised, like with value_counts\n",
    "                init_covariate_supervised[indices_of_new_distribution] = correction_model.n_mix_comp - 1\n",
    "                #print(init_covariate_supervised[indices_of_new_distribution])\n",
    "            with torch.no_grad():\n",
    "                cov_means = torch.zeros((len(data_loader.dataset), correction_model.dim))\n",
    "                for i in range(len(data_loader.dataset)):\n",
    "                    cov_means[i,:] = correction_model.mean[init_covariate_supervised[i],:].clone().detach().cpu()\n",
    "            test_correction_rep = RepresentationLayer(\n",
    "                n_rep=2, n_sample=n_samples_new, value_init=cov_means\n",
    "            ).to(device)\n",
    "        else:\n",
    "            test_correction_rep = RepresentationLayer(\n",
    "                n_rep=2, n_sample=n_samples_new, value_init=\"zero\"\n",
    "            ).to(device)\n",
    "        correction_rep_optim = torch.optim.Adam(\n",
    "            test_correction_rep.parameters(), lr=lrs[0], weight_decay=1e-4, betas=(0.5, 0.7)\n",
    "        )\n",
    "        if correction_hook:\n",
    "            correction_model_optim = torch.optim.Adam(\n",
    "            correction_model.parameters(), lr=lrs[1], weight_decay=0, betas=(0.5, 0.7)\n",
    "            )\n",
    "\n",
    "    rep_init_values = None\n",
    "\n",
    "    #####################\n",
    "    # training reps (only)\n",
    "    #####################\n",
    "    \"\"\"\n",
    "    supervised_warmup = 20\n",
    "    if supervised and (init_covariate_supervised is not None):\n",
    "        # fine-tune the relevant neurons in the first layer of the decoder\n",
    "        for param in decoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # I want to finetune the last neurons of the first layer, corresponding to the covariate model (correction_rep input)\n",
    "        indices_covariate_input = np.arange(new_rep.n_rep, new_rep.n_rep + test_correction_rep.n_rep)\n",
    "        # print the old weights\n",
    "        #print(\"old weights: \", decoder.main[0].weight[:, indices_covariate_input])\n",
    "        decoder.main[0].weight.requires_grad = True\n",
    "        decoder.main[0].bias.requires_grad = True # did not exist for first version\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lrs[0]/100, weight_decay=1e-4, betas=(0.5,0.7)) # was /100 for first version\n",
    "    \"\"\"\n",
    "    print(\"training selected reps for \", n_epochs, \" epochs\")\n",
    "    for epoch in range(n_epochs):\n",
    "        newrep_optimizer.zero_grad()\n",
    "        if correction_model is not None:\n",
    "            correction_rep_optim.zero_grad()\n",
    "        corr_loss = 0\n",
    "        e_loss = 0\n",
    "        recon_loss = 0\n",
    "        for x, lib, i in data_loader:\n",
    "            if correction_hook:\n",
    "                correction_model_optim.zero_grad()\n",
    "                #if supervised:\n",
    "                #    decoder_optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            lib = lib.to(device)\n",
    "            if correction_model is not None:\n",
    "                z = new_rep(i)\n",
    "                z_correction = test_correction_rep(i)\n",
    "                y = decoder(torch.cat((z, z_correction), dim=1))\n",
    "            else:\n",
    "                z = new_rep(i)\n",
    "                y = decoder(z)\n",
    "            # compute losses\n",
    "            if data_loader.dataset.modality_switch is not None:\n",
    "                recon_loss_x = decoder.loss(\n",
    "                    y,\n",
    "                    [x[:, : data_loader.dataset.modality_switch], x[:, data_loader.dataset.modality_switch :]],\n",
    "                    scale=[lib[:, xxx].unsqueeze(1) for xxx in range(decoder.n_out_groups)],\n",
    "                    mask=data_loader.dataset.get_mask(i)\n",
    "                )\n",
    "            else:\n",
    "                recon_loss_x = decoder.loss(\n",
    "                    y, [x], scale=[lib[:, xxx].unsqueeze(1) for xxx in range(decoder.n_out_groups)],\n",
    "                    mask=data_loader.dataset.get_mask(i)\n",
    "                )\n",
    "            # gmm_error = gmm.forward_split(gmm,z).sum()\n",
    "            gmm_error = gmm(z).sum()\n",
    "            correction_error = torch.zeros(1).to(device)\n",
    "            if correction_model is not None:\n",
    "                if supervised:\n",
    "                    # get the component whose mean was used to initialize each sample\n",
    "                    supervision_idx = init_covariate_supervised[i]\n",
    "                    correction_error += correction_model(z_correction, supervision_idx).sum()\n",
    "                else:\n",
    "                    correction_error += correction_model(z_correction).sum()\n",
    "                corr_loss += correction_error.item()\n",
    "            if include_correction_error:\n",
    "                loss = recon_loss_x.clone() + gmm_error.clone() + correction_error.clone() * cov_beta\n",
    "            else:\n",
    "                loss = recon_loss_x.clone() + gmm_error.clone()\n",
    "            loss.backward()\n",
    "            if correction_hook:\n",
    "                correction_model.mean.grad[:n_correction_classes_old,:] = 0\n",
    "                correction_model.neglogvar.grad[:n_correction_classes_old,:] = 0\n",
    "                correction_model.weight.grad[:n_correction_classes_old] = 0\n",
    "                correction_model_optim.step()\n",
    "                #if supervised and (epoch > supervised_warmup):\n",
    "                #    decoder.main[0].weight.grad[:, :new_rep.n_rep] = 0\n",
    "                #    decoder_optimizer.step()\n",
    "            e_loss += loss.item()\n",
    "            recon_loss += recon_loss_x.clone().item()\n",
    "\n",
    "        newrep_optimizer.step()\n",
    "        if correction_model is not None:\n",
    "            correction_rep_optim.step()\n",
    "        e_loss /= (len(data_loader.dataset)*data_loader.dataset.n_features)\n",
    "        recon_loss /= (len(data_loader.dataset)*data_loader.dataset.n_features)\n",
    "        if correction_model is not None:\n",
    "            corr_loss /= (len(data_loader.dataset)*data_loader.dataset.n_features)\n",
    "            print(\"epoch: \", epoch, \" loss: \", e_loss, \" recon: \", recon_loss, \" corr: \", corr_loss)\n",
    "        else:\n",
    "            print(\"epoch: \", epoch, \" loss: \", e_loss, \" recon: \", recon_loss)\n",
    "    \n",
    "    #with torch.no_grad():\n",
    "    #    print(\"new weights: \", decoder.main[0].weight[:, indices_covariate_input])\n",
    "    \n",
    "    if correction_hook:\n",
    "        if supervised:\n",
    "            return decoder, new_rep, test_correction_rep, correction_model\n",
    "        return None, new_rep, test_correction_rep, correction_model\n",
    "    else:\n",
    "        return None, new_rep, test_correction_rep, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check: modality names are  ['GEX', 'ATAC']\n",
      "   all potential reps:  torch.Size([88, 22])\n",
      "calculating losses for each new sample and potential reps\n",
      "   rep_init_values:  torch.Size([6925, 22])\n",
      "    torch.Size([22]) torch.Size([6925])\n",
      "   counts of how often each component has been chosen:  (array([-0.08899279, -0.02229357, -0.01778638, -0.01645249, -0.0154126 ,\n",
      "       -0.01352611, -0.0122846 ,  0.05693668,  0.05827057,  0.05931045,\n",
      "        0.06119695], dtype=float32), array([  60,    2,  942, 5320,  534,    1,    6,   22,   21,   16,    1]))\n",
      "training selected reps for  10  epochs\n",
      "epoch:  0  loss:  0.21522441671510625  recon:  0.21107790882056984  corr:  8.265609951379e-05\n",
      "epoch:  1  loss:  0.21155740183280497  recon:  0.20739153268633087  corr:  8.280899092986277e-05\n",
      "epoch:  2  loss:  0.20821144059857313  recon:  0.204012878091121  corr:  8.311597839598633e-05\n",
      "epoch:  3  loss:  0.2052402194660532  recon:  0.2009864627155958  corr:  8.363679819810628e-05\n",
      "epoch:  4  loss:  0.20266228059737051  recon:  0.1983361992185504  corr:  8.431616922653816e-05\n",
      "epoch:  5  loss:  0.2004637653989371  recon:  0.19605508699058916  corr:  8.513967355823904e-05\n",
      "epoch:  6  loss:  0.1985119821721292  recon:  0.19401185476722774  corr:  8.612621516784385e-05\n",
      "epoch:  7  loss:  0.19678794694231988  recon:  0.19218717797530224  corr:  8.725453470679207e-05\n",
      "epoch:  8  loss:  0.19527033556138823  recon:  0.19055911989470023  corr:  8.858181864290245e-05\n",
      "epoch:  9  loss:  0.19395420632003335  recon:  0.18912444428358116  corr:  9.011811335177116e-05\n",
      "   test set inferred\n"
     ]
    }
   ],
   "source": [
    "original_name = model._model_name\n",
    "# change the model name (because we did inference once for 10 epochs and once for 50)\n",
    "model._model_name = original_name + \"_test10e\"\n",
    "model.predict_new(testset)\n",
    "print(\"   test set inferred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making potential reps\n",
      "   all potential reps:  torch.Size([88, 22])\n",
      "calculating losses for each new sample and potential reps\n",
      "training selected reps for  10  epochs\n",
      "epoch:  0  loss:  0.30016952074764014\n",
      "epoch:  1  loss:  0.2942641044195227\n",
      "epoch:  2  loss:  0.28908638261415565\n",
      "epoch:  3  loss:  0.28476411996979445\n",
      "epoch:  4  loss:  0.2813018356517514\n",
      "epoch:  5  loss:  0.27835773076151776\n",
      "epoch:  6  loss:  0.27579175957580665\n",
      "epoch:  7  loss:  0.27352224981477913\n",
      "epoch:  8  loss:  0.27152744189144434\n",
      "epoch:  9  loss:  0.26980502880297963\n",
      "   test set inferred\n"
     ]
    }
   ],
   "source": [
    "original_name = model._model_name\n",
    "# change the model name (because we did inference once for 10 epochs and once for 50)\n",
    "model._model_name = original_name + \"_test10e_old\"\n",
    "model.predict_new(testset)\n",
    "print(\"   test set inferred\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multidgd-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
