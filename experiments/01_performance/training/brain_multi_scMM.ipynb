{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## I executed this in colab and transferred the trained models to the results section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7aAnAVC1PN0",
        "outputId": "6f38b70d-51f3-42dc-e43b-864b42557dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting mudata\n",
            "  Downloading mudata-0.2.3-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mudata) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mudata) (1.5.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from mudata) (3.8.0)\n",
            "Collecting anndata>=0.8 (from mudata)\n",
            "  Downloading anndata-0.9.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->mudata) (1.10.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->mudata) (8.3.1)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->mudata) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mudata) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mudata) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->mudata) (1.16.0)\n",
            "Installing collected packages: anndata, mudata\n",
            "Successfully installed anndata-0.9.1 mudata-0.2.3\n",
            "Requirement already satisfied: anndata in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pandas>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.22.4)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.10.1)\n",
            "Requirement already satisfied: h5py>=3 in /usr/local/lib/python3.10/dist-packages (from anndata) (3.8.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata) (8.3.1)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from anndata) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.1->anndata) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.1->anndata) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.1->anndata) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('drive/MyDrive/Work/multi-omics/')\n",
        "\n",
        "! pip install mudata\n",
        "import mudata as md\n",
        "from mudata import MuData\n",
        "! pip install anndata\n",
        "import anndata as ad\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuM9IvYLoYQp"
      },
      "source": [
        "## preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Gd0ok9oaQ_"
      },
      "source": [
        "### brain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "5taCSQjcy1wm",
        "outputId": "b421ec56-d5a8-437d-df18-5a29d1724438"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre>MuData object with n_obs × n_vars = 3143 × 110849\n",
              "  obs:\t&#x27;celltype&#x27;\n",
              "  var:\t&#x27;name&#x27;, &#x27;modality&#x27;\n",
              "  2 modalities\n",
              "    rna:\t3143 x 15172\n",
              "      obs:\t&#x27;ID&#x27;, &#x27;rna_celltype&#x27;, &#x27;atac_celltype&#x27;\n",
              "      var:\t&#x27;name&#x27;, &#x27;modality&#x27;\n",
              "    atac:\t3143 x 95677\n",
              "      obs:\t&#x27;ID&#x27;, &#x27;rna_celltype&#x27;, &#x27;atac_celltype&#x27;\n",
              "      var:\t&#x27;name&#x27;, &#x27;modality&#x27;</pre>"
            ],
            "text/plain": [
              "MuData object with n_obs × n_vars = 3143 × 110849\n",
              "  obs:\t'celltype'\n",
              "  var:\t'name', 'modality'\n",
              "  2 modalities\n",
              "    rna:\t3143 x 15172\n",
              "      obs:\t'ID', 'rna_celltype', 'atac_celltype'\n",
              "      var:\t'name', 'modality'\n",
              "    atac:\t3143 x 95677\n",
              "      obs:\t'ID', 'rna_celltype', 'atac_celltype'\n",
              "      var:\t'name', 'modality'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get data\n",
        "data_prefix = './brain/'\n",
        "mdata = md.read(data_prefix+'mudata.h5mu', backed=False)\n",
        "# load train_val_test split for fairness\n",
        "import pandas as pd\n",
        "split = pd.read_csv(data_prefix+'train_val_test_split.csv')\n",
        "#train_indices = split[split['is_train'] == 'train']['num_idx'].values\n",
        "not_test_indices = split[split['is_train'] != 'iid_holdout']['num_idx'].values\n",
        "#mdata = mdata[not_test_indices,:]\n",
        "mdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLF44YahJkGn"
      },
      "outputs": [],
      "source": [
        "# now prepare data for the model\n",
        "from scipy.io import mmwrite\n",
        "# RNA_count.mtx\n",
        "mmwrite(data_prefix+'RNA-seq/RNA_count.mtx', mdata['rna'].X.T)\n",
        "# gene.tsv\n",
        "mdata['rna'].var.to_csv(data_prefix+'RNA-seq/gene.tsv', header=False)\n",
        "# barcode.tsv\n",
        "mdata['rna'].obs.to_csv(data_prefix+'RNA-seq/barcode.tsv', header=False)\n",
        "# ATAC_count.mtx\n",
        "mmwrite(data_prefix+'ATAC-seq/ATAC_count.mtx', mdata['atac'].X.T)\n",
        "# peak.tsv\n",
        "mdata['atac'].var.to_csv(data_prefix+'ATAC-seq/peak.tsv', header=False)\n",
        "# barcode.tsv\n",
        "mdata['atac'].obs.to_csv(data_prefix+'ATAC-seq/barcode.tsv', header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGgkrVvAVahJ",
        "outputId": "92fc5cc5-7905-4267-e853-8185c2b955b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3143, 95677)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mdata['atac'].X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBCPn1X2VeRA",
        "outputId": "52db32c4-d6f1-4a86-f643-1b4ca2672d4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "95677"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(mdata['atac'].var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCmO22ixVgxW",
        "outputId": "f7f76bb0-3373-4ad1-a4a5-dab2465810e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3143"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(mdata['atac'].obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOgA77whVpzT",
        "outputId": "3ad8f904-5d3c-410e-88d1-3546e5a2185c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3143, 15172)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mdata['rna'].X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOGIiFd8Vsbo",
        "outputId": "61f8ab43-a13b-454f-a497-51318810f362"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15172"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(mdata['rna'].var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfYlzR5EVs1D",
        "outputId": "7ff6aa73-39a1-42cf-b49e-78670a1cbd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3143"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(mdata['rna'].obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lVfvLjyy55W"
      },
      "source": [
        "# scMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiorUmK7TI0z",
        "outputId": "893b8982-da7c-4aab-baca-e4146650fd96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchnet\n",
            "  Downloading torchnet-0.0.4.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyro-ppl\n",
            "  Downloading pyro_ppl-1.8.5-py3-none-any.whl (732 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.5/732.5 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchnet) (2.0.1+cu118)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchnet) (1.16.0)\n",
            "Collecting visdom (from torchnet)\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (3.3.0)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchnet) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchnet) (16.0.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (2.27.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (6.3.1)\n",
            "Collecting jsonpatch (from visdom->torchnet)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.6.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchnet) (2.1.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch->visdom->torchnet)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchnet) (1.3.0)\n",
            "Building wheels for collected packages: torchnet, visdom\n",
            "  Building wheel for torchnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchnet: filename=torchnet-0.0.4-py3-none-any.whl size=29728 sha256=8b38f759f6785dd989c24d19dfb44369c1dc45beaab1a0409b26683d0378ce8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/ae/94/9f5edd6871983f30967ad11d60ef434c3d1b007654de4c8065\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408196 sha256=0c99ee491a37cb76c1e788bd6b97f17242d57864695c023273deb5f43cbca2ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n",
            "Successfully built torchnet visdom\n",
            "Installing collected packages: pyro-api, jsonpointer, jsonpatch, visdom, torchnet, pyro-ppl\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 pyro-api-0.1.2 pyro-ppl-1.8.5 torchnet-0.0.4 visdom-0.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torchnet pyro-ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJCNf3n-TY1c",
        "outputId": "d480256b-5977-44a2-e6ee-dec6815a14fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.10.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.65.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82816 sha256=3070b959c6b95d6a0b627740cf8c9576329fb2d40ab7adc3377e661c4aa4729c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55622 sha256=4e7b51323359bdb32bb828e54720cc78e6fddff42bb6807814797cd272b9fbb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.10 umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v59vQEV74BYY",
        "outputId": "7fa4140d-5206-43ec-edb3-27d312733084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-07-19 15:52:23.682848: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-19 15:52:24.626217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "../experiments/brain/2023-07-19T15:52:33.25243675yia24a\n",
            "Loading  data ...\n",
            "Original data contains 3143 cells x 15172 peaks\n",
            "Finished loading takes 0.25 min\n",
            "Loading  data ...\n",
            "Original data contains 3143 cells x 95677 peaks\n",
            "Finished loading takes 0.65 min\n",
            "RNA-seq shape is (3143, 15172)\n",
            "ATAC-seq shape is (3143, 95677)\n",
            "Namespace(experiment='brain', model='rna_atac', obj='m_elbo_naive_warmup', llik_scaling=1.0, batch_size=64, epochs=100, lr=0.0001, latent_dim=20, num_hidden_layers=1, r_hidden_dim=100, p_hidden_dim=20, pre_trained='', learn_prior=False, analytics=True, print_freq=0, no_cuda=False, seed=0, dataset_path='./brain/', r_dim=15172, p_dim=95677, deterministic_warmup=50, train_val_test_split='train_val_test_split.csv', cuda=True)\n",
            "====> Epoch: 001 Train loss: 61524.6558\n",
            "====>             Test loss: 61096.8686\n",
            "====> Epoch: 002 Train loss: 59299.5926\n",
            "====>             Test loss: 58872.9936\n",
            "====> Epoch: 003 Train loss: 57277.7073\n",
            "====>             Test loss: 57235.2205\n",
            "====> Epoch: 004 Train loss: 55656.3313\n",
            "====>             Test loss: 55637.5876\n",
            "====> Epoch: 005 Train loss: 54169.3844\n",
            "====>             Test loss: 54079.6067\n",
            "====> Epoch: 006 Train loss: 52659.1749\n",
            "====>             Test loss: 52833.5438\n",
            "====> Epoch: 007 Train loss: 51402.6599\n",
            "====>             Test loss: 51607.8822\n",
            "====> Epoch: 008 Train loss: 50191.0680\n",
            "====>             Test loss: 50336.9705\n",
            "====> Epoch: 009 Train loss: 49043.3974\n",
            "====>             Test loss: 49324.9666\n",
            "====> Epoch: 010 Train loss: 48053.7505\n",
            "====>             Test loss: 48459.4156\n",
            "====> Epoch: 011 Train loss: 47104.9141\n",
            "====>             Test loss: 47377.1895\n",
            "====> Epoch: 012 Train loss: 46226.8885\n",
            "====>             Test loss: 46522.7444\n",
            "====> Epoch: 013 Train loss: 45321.0295\n",
            "====>             Test loss: 45515.8575\n",
            "====> Epoch: 014 Train loss: 44570.1173\n",
            "====>             Test loss: 44773.2158\n",
            "====> Epoch: 015 Train loss: 43862.0198\n",
            "====>             Test loss: 44315.0900\n",
            "====> Epoch: 016 Train loss: 43138.8194\n",
            "====>             Test loss: 43608.8256\n",
            "====> Epoch: 017 Train loss: 42549.7085\n",
            "====>             Test loss: 43117.7285\n",
            "====> Epoch: 018 Train loss: 42019.3342\n",
            "====>             Test loss: 42551.4896\n",
            "====> Epoch: 019 Train loss: 41459.7158\n",
            "====>             Test loss: 42185.2548\n",
            "====> Epoch: 020 Train loss: 41053.5327\n",
            "====>             Test loss: 41632.6863\n",
            "====> Epoch: 021 Train loss: 40685.6965\n",
            "====>             Test loss: 41416.4530\n",
            "====> Epoch: 022 Train loss: 40193.1019\n",
            "====>             Test loss: 40986.7174\n",
            "====> Epoch: 023 Train loss: 39925.0445\n",
            "====>             Test loss: 40743.3137\n",
            "====> Epoch: 024 Train loss: 39575.5376\n",
            "====>             Test loss: 40434.8312\n",
            "====> Epoch: 025 Train loss: 39227.1808\n",
            "====>             Test loss: 40037.2428\n",
            "====> Epoch: 026 Train loss: 39054.6584\n",
            "====>             Test loss: 39891.8583\n",
            "====> Epoch: 027 Train loss: 38812.9948\n",
            "====>             Test loss: 39651.1306\n",
            "====> Epoch: 028 Train loss: 38521.9526\n",
            "====>             Test loss: 39477.2182\n",
            "====> Epoch: 029 Train loss: 38385.8039\n",
            "====>             Test loss: 39163.8957\n",
            "====> Epoch: 030 Train loss: 38208.0568\n",
            "====>             Test loss: 39202.4427\n",
            "====> Epoch: 031 Train loss: 38016.1401\n",
            "====>             Test loss: 38908.6170\n",
            "====> Epoch: 032 Train loss: 37833.4232\n",
            "====>             Test loss: 38850.7420\n",
            "====> Epoch: 033 Train loss: 37682.9325\n",
            "====>             Test loss: 38649.2580\n",
            "====> Epoch: 034 Train loss: 37524.9282\n",
            "====>             Test loss: 38523.0255\n",
            "====> Epoch: 035 Train loss: 37451.8929\n",
            "====>             Test loss: 38376.3662\n",
            "====> Epoch: 036 Train loss: 37343.1550\n",
            "====>             Test loss: 38247.8877\n",
            "====> Epoch: 037 Train loss: 37231.7804\n",
            "====>             Test loss: 38257.2118\n",
            "====> Epoch: 038 Train loss: 37164.2190\n",
            "====>             Test loss: 38011.4260\n",
            "====> Epoch: 039 Train loss: 37020.8475\n",
            "====>             Test loss: 38043.8065\n",
            "====> Epoch: 040 Train loss: 36898.6439\n",
            "====>             Test loss: 37929.3280\n",
            "====> Epoch: 041 Train loss: 36875.4375\n",
            "====>             Test loss: 37853.4952\n",
            "====> Epoch: 042 Train loss: 36790.8371\n",
            "====>             Test loss: 37808.9076\n",
            "====> Epoch: 043 Train loss: 36665.6317\n",
            "====>             Test loss: 37592.1592\n",
            "====> Epoch: 044 Train loss: 36596.1056\n",
            "====>             Test loss: 37613.0876\n",
            "====> Epoch: 045 Train loss: 36599.8013\n",
            "====>             Test loss: 37586.3623\n",
            "====> Epoch: 046 Train loss: 36516.0601\n",
            "====>             Test loss: 37508.4124\n",
            "====> Epoch: 047 Train loss: 36449.7788\n",
            "====>             Test loss: 37533.3941\n",
            "====> Epoch: 048 Train loss: 36301.7541\n",
            "====>             Test loss: 37407.1608\n",
            "====> Epoch: 049 Train loss: 36326.5622\n",
            "====>             Test loss: 37336.7803\n",
            "====> Epoch: 050 Train loss: 36230.7016\n",
            "====>             Test loss: 37254.3742\n",
            "====> Epoch: 051 Train loss: 36236.6837\n",
            "====>             Test loss: 37308.6067\n",
            "Validation loss decreased (inf --> 37308.606688).  Saving model ...\n",
            "====> Epoch: 052 Train loss: 36122.8277\n",
            "====>             Test loss: 37305.4952\n",
            "Validation loss decreased (37308.606688 --> 37305.495223).  Saving model ...\n",
            "====> Epoch: 053 Train loss: 36121.9711\n",
            "====>             Test loss: 37157.6600\n",
            "Validation loss decreased (37305.495223 --> 37157.660032).  Saving model ...\n",
            "====> Epoch: 054 Train loss: 36042.2425\n",
            "====>             Test loss: 37091.3432\n",
            "Validation loss decreased (37157.660032 --> 37091.343153).  Saving model ...\n",
            "====> Epoch: 055 Train loss: 36021.8905\n",
            "====>             Test loss: 37075.3702\n",
            "Validation loss decreased (37091.343153 --> 37075.370223).  Saving model ...\n",
            "====> Epoch: 056 Train loss: 35961.7127\n",
            "====>             Test loss: 37004.8002\n",
            "Validation loss decreased (37075.370223 --> 37004.800159).  Saving model ...\n",
            "====> Epoch: 057 Train loss: 35946.2967\n",
            "====>             Test loss: 37011.6720\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 058 Train loss: 35913.2636\n",
            "====>             Test loss: 36882.9809\n",
            "Validation loss decreased (37004.800159 --> 36882.980892).  Saving model ...\n",
            "====> Epoch: 059 Train loss: 35868.1125\n",
            "====>             Test loss: 36912.6433\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 060 Train loss: 35862.7209\n",
            "====>             Test loss: 36811.8201\n",
            "Validation loss decreased (36882.980892 --> 36811.820064).  Saving model ...\n",
            "====> Epoch: 061 Train loss: 35795.7104\n",
            "====>             Test loss: 36846.5693\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 062 Train loss: 35778.4699\n",
            "====>             Test loss: 36874.2691\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 063 Train loss: 35721.1983\n",
            "====>             Test loss: 36807.0764\n",
            "Validation loss decreased (36811.820064 --> 36807.076433).  Saving model ...\n",
            "====> Epoch: 064 Train loss: 35722.4143\n",
            "====>             Test loss: 36756.2094\n",
            "Validation loss decreased (36807.076433 --> 36756.209395).  Saving model ...\n",
            "====> Epoch: 065 Train loss: 35639.8459\n",
            "====>             Test loss: 36706.2520\n",
            "Validation loss decreased (36756.209395 --> 36706.251990).  Saving model ...\n",
            "====> Epoch: 066 Train loss: 35612.1051\n",
            "====>             Test loss: 36689.7357\n",
            "Validation loss decreased (36706.251990 --> 36689.735669).  Saving model ...\n",
            "====> Epoch: 067 Train loss: 35579.0634\n",
            "====>             Test loss: 36702.0311\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 068 Train loss: 35577.1760\n",
            "====>             Test loss: 36676.8814\n",
            "Validation loss decreased (36689.735669 --> 36676.881369).  Saving model ...\n",
            "====> Epoch: 069 Train loss: 35573.9189\n",
            "====>             Test loss: 36627.7986\n",
            "Validation loss decreased (36676.881369 --> 36627.798567).  Saving model ...\n",
            "====> Epoch: 070 Train loss: 35497.4461\n",
            "====>             Test loss: 36605.5557\n",
            "Validation loss decreased (36627.798567 --> 36605.555732).  Saving model ...\n",
            "====> Epoch: 071 Train loss: 35455.8405\n",
            "====>             Test loss: 36546.9988\n",
            "Validation loss decreased (36605.555732 --> 36546.998806).  Saving model ...\n",
            "====> Epoch: 072 Train loss: 35448.8613\n",
            "====>             Test loss: 36499.3587\n",
            "Validation loss decreased (36546.998806 --> 36499.358678).  Saving model ...\n",
            "====> Epoch: 073 Train loss: 35461.5963\n",
            "====>             Test loss: 36476.9459\n",
            "Validation loss decreased (36499.358678 --> 36476.945860).  Saving model ...\n",
            "====> Epoch: 074 Train loss: 35443.9708\n",
            "====>             Test loss: 36454.7826\n",
            "Validation loss decreased (36476.945860 --> 36454.782643).  Saving model ...\n",
            "====> Epoch: 075 Train loss: 35343.8549\n",
            "====>             Test loss: 36437.2978\n",
            "Validation loss decreased (36454.782643 --> 36437.297771).  Saving model ...\n",
            "====> Epoch: 076 Train loss: 35390.6638\n",
            "====>             Test loss: 36399.9156\n",
            "Validation loss decreased (36437.297771 --> 36399.915605).  Saving model ...\n",
            "====> Epoch: 077 Train loss: 35307.3729\n",
            "====>             Test loss: 36388.3941\n",
            "Validation loss decreased (36399.915605 --> 36388.394108).  Saving model ...\n",
            "====> Epoch: 078 Train loss: 35275.8742\n",
            "====>             Test loss: 36394.3790\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 079 Train loss: 35252.8420\n",
            "====>             Test loss: 36348.9172\n",
            "Validation loss decreased (36388.394108 --> 36348.917197).  Saving model ...\n",
            "====> Epoch: 080 Train loss: 35267.0327\n",
            "====>             Test loss: 36286.2592\n",
            "Validation loss decreased (36348.917197 --> 36286.259156).  Saving model ...\n",
            "====> Epoch: 081 Train loss: 35268.8065\n",
            "====>             Test loss: 36296.3149\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 082 Train loss: 35164.2231\n",
            "====>             Test loss: 36275.4327\n",
            "Validation loss decreased (36286.259156 --> 36275.432723).  Saving model ...\n",
            "====> Epoch: 083 Train loss: 35215.2158\n",
            "====>             Test loss: 36292.3639\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 084 Train loss: 35182.9245\n",
            "====>             Test loss: 36275.7062\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 085 Train loss: 35153.9388\n",
            "====>             Test loss: 36164.0223\n",
            "Validation loss decreased (36275.432723 --> 36164.022293).  Saving model ...\n",
            "====> Epoch: 086 Train loss: 35122.3088\n",
            "====>             Test loss: 36178.3185\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 087 Train loss: 35167.1720\n",
            "====>             Test loss: 36227.2412\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 088 Train loss: 35166.6525\n",
            "====>             Test loss: 36180.9021\n",
            "EarlyStopping counter: 3 out of 10\n",
            "====> Epoch: 089 Train loss: 35105.0938\n",
            "====>             Test loss: 36234.0223\n",
            "EarlyStopping counter: 4 out of 10\n",
            "====> Epoch: 090 Train loss: 35092.1652\n",
            "====>             Test loss: 36126.8197\n",
            "Validation loss decreased (36164.022293 --> 36126.819666).  Saving model ...\n",
            "====> Epoch: 091 Train loss: 35043.2230\n",
            "====>             Test loss: 36051.5963\n",
            "Validation loss decreased (36126.819666 --> 36051.596338).  Saving model ...\n",
            "====> Epoch: 092 Train loss: 35035.0766\n",
            "====>             Test loss: 36124.8300\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 093 Train loss: 35079.8753\n",
            "====>             Test loss: 36058.5816\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 094 Train loss: 34977.6572\n",
            "====>             Test loss: 36056.3854\n",
            "EarlyStopping counter: 3 out of 10\n",
            "====> Epoch: 095 Train loss: 34975.5520\n",
            "====>             Test loss: 36085.3264\n",
            "EarlyStopping counter: 4 out of 10\n",
            "====> Epoch: 096 Train loss: 34946.0642\n",
            "====>             Test loss: 36045.4574\n",
            "Validation loss decreased (36051.596338 --> 36045.457404).  Saving model ...\n",
            "====> Epoch: 097 Train loss: 34948.7454\n",
            "====>             Test loss: 36032.2114\n",
            "Validation loss decreased (36045.457404 --> 36032.211385).  Saving model ...\n",
            "====> Epoch: 098 Train loss: 34912.8403\n",
            "====>             Test loss: 36010.8002\n",
            "Validation loss decreased (36032.211385 --> 36010.800159).  Saving model ...\n",
            "====> Epoch: 099 Train loss: 34922.9069\n",
            "====>             Test loss: 36012.9383\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 100 Train loss: 34892.9378\n",
            "====>             Test loss: 35985.4765\n",
            "Validation loss decreased (36010.800159 --> 35985.476513).  Saving model ...\n",
            "====> [MM-VAE] Time: 526.702s or 00:08:46\n"
          ]
        }
      ],
      "source": [
        "!python scMM/src/main.py --experiment 'brain' --model 'rna_atac' --epochs 100 --dataset_path './brain/' --train_val_test_split 'train_val_test_split.csv' --seed 0 --latent_dim 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skF18GY82-23",
        "outputId": "1f37f737-ac0b-4be4-fca8-9f2fc8039c3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-07-19 16:02:57.055929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-19 16:02:58.764115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "../experiments/brain/2023-07-19T16:03:01.330121b5b7mfmh\n",
            "Loading  data ...\n",
            "Original data contains 3143 cells x 15172 peaks\n",
            "Finished loading takes 0.14 min\n",
            "Loading  data ...\n",
            "Original data contains 3143 cells x 95677 peaks\n",
            "Finished loading takes 0.53 min\n",
            "RNA-seq shape is (3143, 15172)\n",
            "ATAC-seq shape is (3143, 95677)\n",
            "Namespace(experiment='brain', model='rna_atac', obj='m_elbo_naive_warmup', llik_scaling=1.0, batch_size=64, epochs=100, lr=0.0001, latent_dim=20, num_hidden_layers=1, r_hidden_dim=100, p_hidden_dim=20, pre_trained='', learn_prior=False, analytics=True, print_freq=0, no_cuda=False, seed=37, dataset_path='./brain/', r_dim=15172, p_dim=95677, deterministic_warmup=50, train_val_test_split='train_val_test_split.csv', cuda=True)\n",
            "====> Epoch: 001 Train loss: 61496.5029\n",
            "====>             Test loss: 61155.0486\n",
            "====> Epoch: 002 Train loss: 59153.8302\n",
            "====>             Test loss: 58834.9451\n",
            "====> Epoch: 003 Train loss: 57152.9236\n",
            "====>             Test loss: 57146.0685\n",
            "====> Epoch: 004 Train loss: 55609.7038\n",
            "====>             Test loss: 55596.5709\n",
            "====> Epoch: 005 Train loss: 53972.5685\n",
            "====>             Test loss: 53962.4411\n",
            "====> Epoch: 006 Train loss: 52637.5227\n",
            "====>             Test loss: 52692.3893\n",
            "====> Epoch: 007 Train loss: 51297.4004\n",
            "====>             Test loss: 51450.9045\n",
            "====> Epoch: 008 Train loss: 50073.6433\n",
            "====>             Test loss: 50168.4514\n",
            "====> Epoch: 009 Train loss: 48938.9429\n",
            "====>             Test loss: 49133.5796\n",
            "====> Epoch: 010 Train loss: 47848.2227\n",
            "====>             Test loss: 48036.2022\n",
            "====> Epoch: 011 Train loss: 46823.3416\n",
            "====>             Test loss: 47211.0287\n",
            "====> Epoch: 012 Train loss: 45883.2578\n",
            "====>             Test loss: 46332.9546\n",
            "====> Epoch: 013 Train loss: 45040.7591\n",
            "====>             Test loss: 45533.9283\n",
            "====> Epoch: 014 Train loss: 44181.4163\n",
            "====>             Test loss: 44805.1712\n",
            "====> Epoch: 015 Train loss: 43550.1391\n",
            "====>             Test loss: 44279.5549\n",
            "====> Epoch: 016 Train loss: 42824.3909\n",
            "====>             Test loss: 43511.3718\n",
            "====> Epoch: 017 Train loss: 42217.7164\n",
            "====>             Test loss: 43077.2341\n",
            "====> Epoch: 018 Train loss: 41688.3856\n",
            "====>             Test loss: 42524.1704\n",
            "====> Epoch: 019 Train loss: 41103.8057\n",
            "====>             Test loss: 42125.6911\n",
            "====> Epoch: 020 Train loss: 40678.5241\n",
            "====>             Test loss: 41712.0685\n",
            "====> Epoch: 021 Train loss: 40178.0754\n",
            "====>             Test loss: 41285.8686\n",
            "====> Epoch: 022 Train loss: 39873.2917\n",
            "====>             Test loss: 41023.4769\n",
            "====> Epoch: 023 Train loss: 39467.8191\n",
            "====>             Test loss: 40645.2349\n",
            "====> Epoch: 024 Train loss: 39184.0641\n",
            "====>             Test loss: 40557.7237\n",
            "====> Epoch: 025 Train loss: 38928.1612\n",
            "====>             Test loss: 40287.7245\n",
            "====> Epoch: 026 Train loss: 38642.5793\n",
            "====>             Test loss: 39962.5709\n",
            "====> Epoch: 027 Train loss: 38431.1211\n",
            "====>             Test loss: 39751.5342\n",
            "====> Epoch: 028 Train loss: 38199.5170\n",
            "====>             Test loss: 39466.6346\n",
            "====> Epoch: 029 Train loss: 38016.9053\n",
            "====>             Test loss: 39361.5995\n",
            "====> Epoch: 030 Train loss: 37797.8819\n",
            "====>             Test loss: 39233.6903\n",
            "====> Epoch: 031 Train loss: 37576.6650\n",
            "====>             Test loss: 39116.1584\n",
            "====> Epoch: 032 Train loss: 37447.1131\n",
            "====>             Test loss: 38896.8121\n",
            "====> Epoch: 033 Train loss: 37369.1378\n",
            "====>             Test loss: 38882.4666\n",
            "====> Epoch: 034 Train loss: 37237.4083\n",
            "====>             Test loss: 38809.8304\n",
            "====> Epoch: 035 Train loss: 37077.2637\n",
            "====>             Test loss: 38541.5685\n",
            "====> Epoch: 036 Train loss: 36904.3590\n",
            "====>             Test loss: 38386.7834\n",
            "====> Epoch: 037 Train loss: 36937.9308\n",
            "====>             Test loss: 38441.4291\n",
            "====> Epoch: 038 Train loss: 36769.5230\n",
            "====>             Test loss: 38310.4252\n",
            "====> Epoch: 039 Train loss: 36692.4783\n",
            "====>             Test loss: 38164.8089\n",
            "====> Epoch: 040 Train loss: 36633.7060\n",
            "====>             Test loss: 38039.8384\n",
            "====> Epoch: 041 Train loss: 36453.2713\n",
            "====>             Test loss: 37972.7970\n",
            "====> Epoch: 042 Train loss: 36445.9653\n",
            "====>             Test loss: 37883.9873\n",
            "====> Epoch: 043 Train loss: 36383.7539\n",
            "====>             Test loss: 37895.1537\n",
            "====> Epoch: 044 Train loss: 36338.4124\n",
            "====>             Test loss: 37822.6505\n",
            "====> Epoch: 045 Train loss: 36185.3261\n",
            "====>             Test loss: 37735.4912\n",
            "====> Epoch: 046 Train loss: 36188.6215\n",
            "====>             Test loss: 37581.4228\n",
            "====> Epoch: 047 Train loss: 36153.2197\n",
            "====>             Test loss: 37555.6322\n",
            "====> Epoch: 048 Train loss: 36057.0131\n",
            "====>             Test loss: 37544.6393\n",
            "====> Epoch: 049 Train loss: 36023.8701\n",
            "====>             Test loss: 37409.8678\n",
            "====> Epoch: 050 Train loss: 35963.1387\n",
            "====>             Test loss: 37374.7420\n",
            "====> Epoch: 051 Train loss: 35912.4037\n",
            "====>             Test loss: 37284.2404\n",
            "Validation loss decreased (inf --> 37284.240446).  Saving model ...\n",
            "====> Epoch: 052 Train loss: 35888.9830\n",
            "====>             Test loss: 37285.1330\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 053 Train loss: 35854.1630\n",
            "====>             Test loss: 37196.4817\n",
            "Validation loss decreased (37284.240446 --> 37196.481688).  Saving model ...\n",
            "====> Epoch: 054 Train loss: 35798.2612\n",
            "====>             Test loss: 37196.5470\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 055 Train loss: 35745.9400\n",
            "====>             Test loss: 37122.8336\n",
            "Validation loss decreased (37196.481688 --> 37122.833599).  Saving model ...\n",
            "====> Epoch: 056 Train loss: 35730.7315\n",
            "====>             Test loss: 37069.4331\n",
            "Validation loss decreased (37122.833599 --> 37069.433121).  Saving model ...\n",
            "====> Epoch: 057 Train loss: 35695.8793\n",
            "====>             Test loss: 37006.2046\n",
            "Validation loss decreased (37069.433121 --> 37006.204618).  Saving model ...\n",
            "====> Epoch: 058 Train loss: 35638.9423\n",
            "====>             Test loss: 36962.4578\n",
            "Validation loss decreased (37006.204618 --> 36962.457803).  Saving model ...\n",
            "====> Epoch: 059 Train loss: 35617.6615\n",
            "====>             Test loss: 36982.9188\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 060 Train loss: 35584.6270\n",
            "====>             Test loss: 36914.3447\n",
            "Validation loss decreased (36962.457803 --> 36914.344745).  Saving model ...\n",
            "====> Epoch: 061 Train loss: 35557.3087\n",
            "====>             Test loss: 36942.5494\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 062 Train loss: 35469.1653\n",
            "====>             Test loss: 36898.8173\n",
            "Validation loss decreased (36914.344745 --> 36898.817277).  Saving model ...\n",
            "====> Epoch: 063 Train loss: 35507.2640\n",
            "====>             Test loss: 36760.6839\n",
            "Validation loss decreased (36898.817277 --> 36760.683917).  Saving model ...\n",
            "====> Epoch: 064 Train loss: 35520.3609\n",
            "====>             Test loss: 36718.1807\n",
            "Validation loss decreased (36760.683917 --> 36718.180732).  Saving model ...\n",
            "====> Epoch: 065 Train loss: 35414.7550\n",
            "====>             Test loss: 36770.7938\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 066 Train loss: 35396.2853\n",
            "====>             Test loss: 36714.5498\n",
            "Validation loss decreased (36718.180732 --> 36714.549761).  Saving model ...\n",
            "====> Epoch: 067 Train loss: 35436.6266\n",
            "====>             Test loss: 36655.5916\n",
            "Validation loss decreased (36714.549761 --> 36655.591561).  Saving model ...\n",
            "====> Epoch: 068 Train loss: 35372.1864\n",
            "====>             Test loss: 36634.6752\n",
            "Validation loss decreased (36655.591561 --> 36634.675159).  Saving model ...\n",
            "====> Epoch: 069 Train loss: 35356.2289\n",
            "====>             Test loss: 36571.7528\n",
            "Validation loss decreased (36634.675159 --> 36571.752787).  Saving model ...\n",
            "====> Epoch: 070 Train loss: 35297.0771\n",
            "====>             Test loss: 36538.7086\n",
            "Validation loss decreased (36571.752787 --> 36538.708599).  Saving model ...\n",
            "====> Epoch: 071 Train loss: 35306.7332\n",
            "====>             Test loss: 36512.1744\n",
            "Validation loss decreased (36538.708599 --> 36512.174363).  Saving model ...\n",
            "====> Epoch: 072 Train loss: 35261.5705\n",
            "====>             Test loss: 36503.8025\n",
            "Validation loss decreased (36512.174363 --> 36503.802548).  Saving model ...\n",
            "====> Epoch: 073 Train loss: 35256.0179\n",
            "====>             Test loss: 36472.2269\n",
            "Validation loss decreased (36503.802548 --> 36472.226911).  Saving model ...\n",
            "====> Epoch: 074 Train loss: 35257.2333\n",
            "====>             Test loss: 36422.0510\n",
            "Validation loss decreased (36472.226911 --> 36422.050955).  Saving model ...\n",
            "====> Epoch: 075 Train loss: 35231.4587\n",
            "====>             Test loss: 36377.4885\n",
            "Validation loss decreased (36422.050955 --> 36377.488455).  Saving model ...\n",
            "====> Epoch: 076 Train loss: 35188.4536\n",
            "====>             Test loss: 36386.9427\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 077 Train loss: 35209.7765\n",
            "====>             Test loss: 36338.1513\n",
            "Validation loss decreased (36377.488455 --> 36338.151274).  Saving model ...\n",
            "====> Epoch: 078 Train loss: 35164.9665\n",
            "====>             Test loss: 36322.6019\n",
            "Validation loss decreased (36338.151274 --> 36322.601911).  Saving model ...\n",
            "====> Epoch: 079 Train loss: 35179.9164\n",
            "====>             Test loss: 36320.6222\n",
            "Validation loss decreased (36322.601911 --> 36320.622213).  Saving model ...\n",
            "====> Epoch: 080 Train loss: 35179.5502\n",
            "====>             Test loss: 36343.9558\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 081 Train loss: 35137.6599\n",
            "====>             Test loss: 36278.8770\n",
            "Validation loss decreased (36320.622213 --> 36278.876990).  Saving model ...\n",
            "====> Epoch: 082 Train loss: 35182.6875\n",
            "====>             Test loss: 36294.6795\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 083 Train loss: 35146.6078\n",
            "====>             Test loss: 36235.7062\n",
            "Validation loss decreased (36278.876990 --> 36235.706210).  Saving model ...\n",
            "====> Epoch: 084 Train loss: 35077.1801\n",
            "====>             Test loss: 36187.8201\n",
            "Validation loss decreased (36235.706210 --> 36187.820064).  Saving model ...\n",
            "====> Epoch: 085 Train loss: 35010.3092\n",
            "====>             Test loss: 36229.7154\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 086 Train loss: 35038.7101\n",
            "====>             Test loss: 36189.4829\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 087 Train loss: 35078.9770\n",
            "====>             Test loss: 36126.3814\n",
            "Validation loss decreased (36187.820064 --> 36126.381369).  Saving model ...\n",
            "====> Epoch: 088 Train loss: 35002.0031\n",
            "====>             Test loss: 36202.7651\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 089 Train loss: 34966.1139\n",
            "====>             Test loss: 36133.7253\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 090 Train loss: 35052.8478\n",
            "====>             Test loss: 36106.0338\n",
            "Validation loss decreased (36126.381369 --> 36106.033838).  Saving model ...\n",
            "====> Epoch: 091 Train loss: 34985.7098\n",
            "====>             Test loss: 36039.7914\n",
            "Validation loss decreased (36106.033838 --> 36039.791401).  Saving model ...\n",
            "====> Epoch: 092 Train loss: 34978.5706\n",
            "====>             Test loss: 36072.8543\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 093 Train loss: 34955.6827\n",
            "====>             Test loss: 36069.0609\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 094 Train loss: 34917.3527\n",
            "====>             Test loss: 36013.3193\n",
            "Validation loss decreased (36039.791401 --> 36013.319268).  Saving model ...\n",
            "====> Epoch: 095 Train loss: 34916.6007\n",
            "====>             Test loss: 36001.0888\n",
            "Validation loss decreased (36013.319268 --> 36001.088774).  Saving model ...\n",
            "====> Epoch: 096 Train loss: 34869.5460\n",
            "====>             Test loss: 36008.8205\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 097 Train loss: 34926.4192\n",
            "====>             Test loss: 35976.1994\n",
            "Validation loss decreased (36001.088774 --> 35976.199443).  Saving model ...\n",
            "====> Epoch: 098 Train loss: 34963.2408\n",
            "====>             Test loss: 36018.2122\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 099 Train loss: 34885.9571\n",
            "====>             Test loss: 35981.7934\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 100 Train loss: 34847.5098\n",
            "====>             Test loss: 35897.0032\n",
            "Validation loss decreased (35976.199443 --> 35897.003185).  Saving model ...\n",
            "====> [MM-VAE] Time: 522.126s or 00:08:42\n"
          ]
        }
      ],
      "source": [
        "!python scMM/src/main.py --experiment 'brain' --model 'rna_atac' --epochs 100 --dataset_path './brain/' --train_val_test_split 'train_val_test_split.csv' --seed 37 --latent_dim 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSNvoSwj3AzP",
        "outputId": "30a7db30-3395-4d74-e239-38c3c7cdb259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-07-19 16:12:52.970395: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-19 16:12:53.884469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "../experiments/brain/2023-07-19T16:12:55.533731rf6h7d3t\n",
            "Loading  data ...\n",
            "Original data contains 3143 cells x 15172 peaks\n",
            "Finished loading takes 0.17 min\n",
            "Loading  data ...\n",
            "Original data contains 3143 cells x 95677 peaks\n",
            "Finished loading takes 0.52 min\n",
            "RNA-seq shape is (3143, 15172)\n",
            "ATAC-seq shape is (3143, 95677)\n",
            "Namespace(experiment='brain', model='rna_atac', obj='m_elbo_naive_warmup', llik_scaling=1.0, batch_size=64, epochs=100, lr=0.0001, latent_dim=20, num_hidden_layers=1, r_hidden_dim=100, p_hidden_dim=20, pre_trained='', learn_prior=False, analytics=True, print_freq=0, no_cuda=False, seed=8790, dataset_path='./brain/', r_dim=15172, p_dim=95677, deterministic_warmup=50, train_val_test_split='train_val_test_split.csv', cuda=True)\n",
            "====> Epoch: 001 Train loss: 61520.9730\n",
            "====>             Test loss: 61005.9100\n",
            "====> Epoch: 002 Train loss: 59195.9048\n",
            "====>             Test loss: 58828.6242\n",
            "====> Epoch: 003 Train loss: 57315.6944\n",
            "====>             Test loss: 57234.2118\n",
            "====> Epoch: 004 Train loss: 55662.9916\n",
            "====>             Test loss: 55612.1831\n",
            "====> Epoch: 005 Train loss: 54152.6494\n",
            "====>             Test loss: 54101.5621\n",
            "====> Epoch: 006 Train loss: 52613.3068\n",
            "====>             Test loss: 52732.6672\n",
            "====> Epoch: 007 Train loss: 51356.9724\n",
            "====>             Test loss: 51504.1879\n",
            "====> Epoch: 008 Train loss: 50044.4027\n",
            "====>             Test loss: 50382.9037\n",
            "====> Epoch: 009 Train loss: 48890.9807\n",
            "====>             Test loss: 49264.4761\n",
            "====> Epoch: 010 Train loss: 47904.6997\n",
            "====>             Test loss: 48110.7373\n",
            "====> Epoch: 011 Train loss: 46890.1973\n",
            "====>             Test loss: 47180.8949\n",
            "====> Epoch: 012 Train loss: 45901.3458\n",
            "====>             Test loss: 46421.8033\n",
            "====> Epoch: 013 Train loss: 45108.7224\n",
            "====>             Test loss: 45614.8591\n",
            "====> Epoch: 014 Train loss: 44285.7630\n",
            "====>             Test loss: 44759.8806\n",
            "====> Epoch: 015 Train loss: 43541.4004\n",
            "====>             Test loss: 44232.0796\n",
            "====> Epoch: 016 Train loss: 42852.1908\n",
            "====>             Test loss: 43535.0844\n",
            "====> Epoch: 017 Train loss: 42233.8352\n",
            "====>             Test loss: 42960.9275\n",
            "====> Epoch: 018 Train loss: 41631.4750\n",
            "====>             Test loss: 42383.9785\n",
            "====> Epoch: 019 Train loss: 41047.7967\n",
            "====>             Test loss: 41927.1043\n",
            "====> Epoch: 020 Train loss: 40691.8116\n",
            "====>             Test loss: 41437.2930\n",
            "====> Epoch: 021 Train loss: 40234.8029\n",
            "====>             Test loss: 41116.0621\n",
            "====> Epoch: 022 Train loss: 39822.9338\n",
            "====>             Test loss: 40658.2022\n",
            "====> Epoch: 023 Train loss: 39421.8713\n",
            "====>             Test loss: 40447.1744\n",
            "====> Epoch: 024 Train loss: 39122.5553\n",
            "====>             Test loss: 40123.3989\n",
            "====> Epoch: 025 Train loss: 38848.3765\n",
            "====>             Test loss: 39940.5175\n",
            "====> Epoch: 026 Train loss: 38537.9725\n",
            "====>             Test loss: 39657.6672\n",
            "====> Epoch: 027 Train loss: 38355.1893\n",
            "====>             Test loss: 39428.4522\n",
            "====> Epoch: 028 Train loss: 38105.2568\n",
            "====>             Test loss: 39311.2803\n",
            "====> Epoch: 029 Train loss: 37856.7106\n",
            "====>             Test loss: 39144.4164\n",
            "====> Epoch: 030 Train loss: 37697.0758\n",
            "====>             Test loss: 38906.3814\n",
            "====> Epoch: 031 Train loss: 37537.4116\n",
            "====>             Test loss: 38686.1720\n",
            "====> Epoch: 032 Train loss: 37397.7315\n",
            "====>             Test loss: 38654.8511\n",
            "====> Epoch: 033 Train loss: 37247.6338\n",
            "====>             Test loss: 38545.9729\n",
            "====> Epoch: 034 Train loss: 37114.4164\n",
            "====>             Test loss: 38397.7444\n",
            "====> Epoch: 035 Train loss: 37026.5815\n",
            "====>             Test loss: 38420.1616\n",
            "====> Epoch: 036 Train loss: 36908.9663\n",
            "====>             Test loss: 38228.8503\n",
            "====> Epoch: 037 Train loss: 36764.0404\n",
            "====>             Test loss: 38089.6393\n",
            "====> Epoch: 038 Train loss: 36732.1936\n",
            "====>             Test loss: 38106.8105\n",
            "====> Epoch: 039 Train loss: 36623.0033\n",
            "====>             Test loss: 37973.0533\n",
            "====> Epoch: 040 Train loss: 36567.4716\n",
            "====>             Test loss: 37943.4021\n",
            "====> Epoch: 041 Train loss: 36454.0786\n",
            "====>             Test loss: 37834.3734\n",
            "====> Epoch: 042 Train loss: 36398.4006\n",
            "====>             Test loss: 37850.6067\n",
            "====> Epoch: 043 Train loss: 36359.7747\n",
            "====>             Test loss: 37700.5685\n",
            "====> Epoch: 044 Train loss: 36336.2017\n",
            "====>             Test loss: 37682.5764\n",
            "====> Epoch: 045 Train loss: 36202.1209\n",
            "====>             Test loss: 37699.0311\n",
            "====> Epoch: 046 Train loss: 36158.4757\n",
            "====>             Test loss: 37551.9737\n",
            "====> Epoch: 047 Train loss: 36191.0875\n",
            "====>             Test loss: 37482.1720\n",
            "====> Epoch: 048 Train loss: 36102.1306\n",
            "====>             Test loss: 37450.6775\n",
            "====> Epoch: 049 Train loss: 36032.1252\n",
            "====>             Test loss: 37358.5677\n",
            "====> Epoch: 050 Train loss: 36018.1969\n",
            "====>             Test loss: 37342.2779\n",
            "====> Epoch: 051 Train loss: 35964.7988\n",
            "====>             Test loss: 37309.8766\n",
            "Validation loss decreased (inf --> 37309.876592).  Saving model ...\n",
            "====> Epoch: 052 Train loss: 35904.5492\n",
            "====>             Test loss: 37297.5016\n",
            "Validation loss decreased (37309.876592 --> 37297.501592).  Saving model ...\n",
            "====> Epoch: 053 Train loss: 35879.5720\n",
            "====>             Test loss: 37183.7373\n",
            "Validation loss decreased (37297.501592 --> 37183.737261).  Saving model ...\n",
            "====> Epoch: 054 Train loss: 35863.0045\n",
            "====>             Test loss: 37196.2508\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 055 Train loss: 35805.4367\n",
            "====>             Test loss: 37142.7150\n",
            "Validation loss decreased (37183.737261 --> 37142.714968).  Saving model ...\n",
            "====> Epoch: 056 Train loss: 35789.3901\n",
            "====>             Test loss: 37163.0518\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 057 Train loss: 35826.0330\n",
            "====>             Test loss: 37082.0279\n",
            "Validation loss decreased (37142.714968 --> 37082.027866).  Saving model ...\n",
            "====> Epoch: 058 Train loss: 35761.2116\n",
            "====>             Test loss: 37017.7460\n",
            "Validation loss decreased (37082.027866 --> 37017.746019).  Saving model ...\n",
            "====> Epoch: 059 Train loss: 35695.8895\n",
            "====>             Test loss: 37015.3360\n",
            "Validation loss decreased (37017.746019 --> 37015.335987).  Saving model ...\n",
            "====> Epoch: 060 Train loss: 35670.6184\n",
            "====>             Test loss: 36959.7826\n",
            "Validation loss decreased (37015.335987 --> 36959.782643).  Saving model ...\n",
            "====> Epoch: 061 Train loss: 35646.5756\n",
            "====>             Test loss: 36942.6545\n",
            "Validation loss decreased (36959.782643 --> 36942.654459).  Saving model ...\n",
            "====> Epoch: 062 Train loss: 35650.5311\n",
            "====>             Test loss: 36855.5171\n",
            "Validation loss decreased (36942.654459 --> 36855.517118).  Saving model ...\n",
            "====> Epoch: 063 Train loss: 35587.9782\n",
            "====>             Test loss: 36927.7611\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 064 Train loss: 35584.0195\n",
            "====>             Test loss: 36858.5995\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 065 Train loss: 35612.4173\n",
            "====>             Test loss: 36828.3670\n",
            "Validation loss decreased (36855.517118 --> 36828.367038).  Saving model ...\n",
            "====> Epoch: 066 Train loss: 35549.4529\n",
            "====>             Test loss: 36783.4268\n",
            "Validation loss decreased (36828.367038 --> 36783.426752).  Saving model ...\n",
            "====> Epoch: 067 Train loss: 35517.4435\n",
            "====>             Test loss: 36742.0490\n",
            "Validation loss decreased (36783.426752 --> 36742.048965).  Saving model ...\n",
            "====> Epoch: 068 Train loss: 35412.9763\n",
            "====>             Test loss: 36750.3364\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 069 Train loss: 35467.7201\n",
            "====>             Test loss: 36695.4100\n",
            "Validation loss decreased (36742.048965 --> 36695.410032).  Saving model ...\n",
            "====> Epoch: 070 Train loss: 35493.4506\n",
            "====>             Test loss: 36674.5924\n",
            "Validation loss decreased (36695.410032 --> 36674.592357).  Saving model ...\n",
            "====> Epoch: 071 Train loss: 35468.9342\n",
            "====>             Test loss: 36656.7946\n",
            "Validation loss decreased (36674.592357 --> 36656.794586).  Saving model ...\n",
            "====> Epoch: 072 Train loss: 35372.7251\n",
            "====>             Test loss: 36659.6131\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 073 Train loss: 35421.7705\n",
            "====>             Test loss: 36643.8002\n",
            "Validation loss decreased (36656.794586 --> 36643.800159).  Saving model ...\n",
            "====> Epoch: 074 Train loss: 35416.4763\n",
            "====>             Test loss: 36678.5916\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 075 Train loss: 35372.1022\n",
            "====>             Test loss: 36565.6346\n",
            "Validation loss decreased (36643.800159 --> 36565.634554).  Saving model ...\n",
            "====> Epoch: 076 Train loss: 35408.3201\n",
            "====>             Test loss: 36552.2854\n",
            "Validation loss decreased (36565.634554 --> 36552.285430).  Saving model ...\n",
            "====> Epoch: 077 Train loss: 35301.6830\n",
            "====>             Test loss: 36560.6346\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 078 Train loss: 35332.9125\n",
            "====>             Test loss: 36586.0709\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 079 Train loss: 35322.8954\n",
            "====>             Test loss: 36505.2365\n",
            "Validation loss decreased (36552.285430 --> 36505.236465).  Saving model ...\n",
            "====> Epoch: 080 Train loss: 35320.9619\n",
            "====>             Test loss: 36449.0048\n",
            "Validation loss decreased (36505.236465 --> 36449.004777).  Saving model ...\n",
            "====> Epoch: 081 Train loss: 35272.3573\n",
            "====>             Test loss: 36494.9275\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 082 Train loss: 35301.6797\n",
            "====>             Test loss: 36417.1087\n",
            "Validation loss decreased (36449.004777 --> 36417.108678).  Saving model ...\n",
            "====> Epoch: 083 Train loss: 35237.7971\n",
            "====>             Test loss: 36423.4817\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 084 Train loss: 35248.9436\n",
            "====>             Test loss: 36447.2054\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 085 Train loss: 35248.5580\n",
            "====>             Test loss: 36346.4566\n",
            "Validation loss decreased (36417.108678 --> 36346.456608).  Saving model ...\n",
            "====> Epoch: 086 Train loss: 35230.5541\n",
            "====>             Test loss: 36375.6290\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 087 Train loss: 35195.4261\n",
            "====>             Test loss: 36351.6879\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 088 Train loss: 35181.2486\n",
            "====>             Test loss: 36356.6194\n",
            "EarlyStopping counter: 3 out of 10\n",
            "====> Epoch: 089 Train loss: 35144.4063\n",
            "====>             Test loss: 36314.5251\n",
            "Validation loss decreased (36346.456608 --> 36314.525080).  Saving model ...\n",
            "====> Epoch: 090 Train loss: 35166.8297\n",
            "====>             Test loss: 36341.4279\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 091 Train loss: 35164.1315\n",
            "====>             Test loss: 36243.2476\n",
            "Validation loss decreased (36314.525080 --> 36243.247611).  Saving model ...\n",
            "====> Epoch: 092 Train loss: 35139.6681\n",
            "====>             Test loss: 36294.5255\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 093 Train loss: 35111.0233\n",
            "====>             Test loss: 36234.9045\n",
            "Validation loss decreased (36243.247611 --> 36234.904459).  Saving model ...\n",
            "====> Epoch: 094 Train loss: 35124.6980\n",
            "====>             Test loss: 36239.4626\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 095 Train loss: 35028.8616\n",
            "====>             Test loss: 36158.3850\n",
            "Validation loss decreased (36234.904459 --> 36158.384952).  Saving model ...\n",
            "====> Epoch: 096 Train loss: 35075.4884\n",
            "====>             Test loss: 36167.0844\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 097 Train loss: 35067.0888\n",
            "====>             Test loss: 36172.5506\n",
            "EarlyStopping counter: 2 out of 10\n",
            "====> Epoch: 098 Train loss: 35023.3487\n",
            "====>             Test loss: 36128.4960\n",
            "Validation loss decreased (36158.384952 --> 36128.496019).  Saving model ...\n",
            "====> Epoch: 099 Train loss: 35011.8601\n",
            "====>             Test loss: 36157.5852\n",
            "EarlyStopping counter: 1 out of 10\n",
            "====> Epoch: 100 Train loss: 35015.3321\n",
            "====>             Test loss: 36111.3145\n",
            "Validation loss decreased (36128.496019 --> 36111.314490).  Saving model ...\n",
            "====> [MM-VAE] Time: 512.071s or 00:08:32\n"
          ]
        }
      ],
      "source": [
        "!python scMM/src/main.py --experiment 'brain' --model 'rna_atac' --epochs 100 --dataset_path './brain/' --train_val_test_split 'train_val_test_split.csv' --seed 8790 --latent_dim 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further steps\n",
        "\n",
        "1. download `lat_train_mean.csv`, `pred_test_r_r.mtx` and `pred_test_p_p.mtx` for every run.\n",
        "2. rename them to `scmm_lat_train_mean_rs{seed}.csv`, `scmm_counts_gex_rs{seed}.mtx` and `scmm_counts_atac_rs{seed}.mtx`, respectively.\n",
        "3. save them into `./experiments/results/other_models/scMM/`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "f3Gd0ok9oaQ_"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
